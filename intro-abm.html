<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>intro-abm</title>
<style type="text/css">
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>
<!-- Loading mathjax macro -->
<!-- Load mathjax -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>
</head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Game-of-Life">Game of Life<a class="anchor-link" href="#Game-of-Life">&#182;</a></h1><p>Conway's <em>Game of Life</em> (1970) is a cellular automaton: a
dynamical model/system in which space is discretized into a regular
grid of <strong>cells</strong>; for each cell, different variables can represent
its <strong>state</strong>. In the case of the GoL, there is only one variable
attached to each cell, which can take the values <em>dead</em> (0) or
<em>alive</em> (1).  Thus, the GoL is a <strong>discrete space</strong> model, as are
by definition all cellular automata; moreover, in this model each cell
variable can only have a discrete set of states (two).</p>
<p>Any dynamical system has rules for its <strong>time evolution</strong>, i.e. a
way to specify the state of the system at a time $t' &gt; t$ given the
state of the system at time $t$ (and possibly even previous times, if
the system has <em>memory</em>) -- the word "evolution" here simply
means "change as a function of time". In the case of a cellular
automaton, cells cannot change relative position; they can only change
state. In cellular automata, as well as in agent-based models, such
rules specify which <strong>actions</strong> can be performed.</p>
<p>Take a cellular automaton made of infinite cells distributed on a
rectangular grid in 2 dimensions. Let's define a cell's <em>neighbors</em> 
as all the cells that are horizontally, vertically, or
diagonally adjacent to that cell (this is called <em>Moore
  neighborhood</em>). The rules for GoL also assume <em>discrete time
  steps</em>: the system "jumps" from the initial time $t$ to
$(t + \tau)$, to $(t + \tau + \tau)$, and so on; or, alternatively,
from timestep $T=1$ to $T=2$, then $T=3$ etc.</p>
<p>Here are the rules:</p>
<ul>
<li>any dead cell with 3 alive neighbors becomes alive</li>
<li>any alive cell with $&lt;2$ or $&gt;3$ alive neighbors
becomes dead</li>
<li>all other cells do not change state</li>
</ul>
<p>At each time, the updates of all cells are <strong>independent</strong>.  Once
you specify an initial set of alive cells, the time evolution of
the system is uniquely determined by these rules; as such, GoL is a
<strong>deterministic</strong> (i.e., <strong>not stochastic</strong>) cellular
automaton.</p>
<p>One can already clearly see the difference between <em>behavioral
  rules</em> (that are the same for every cell) and actual <em>behavior</em>
(which depends on internal state and on the environment).</p>
<h4 id="$\Longrightarrow$-See-game_of_life_DT.py-for-an-introductory-implementation.">$\Longrightarrow$ See <code>game_of_life_DT.py</code> for an introductory implementation.<a class="anchor-link" href="#$\Longrightarrow$-See-game_of_life_DT.py-for-an-introductory-implementation.">&#182;</a></h4><h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>what could be the meaning of these rules, if you think
in (very loose) analogy with biological populations?</li>
<li>how fast is the "speed of light" in the GoL
  universe? It is determined by the minimum time interval after
  which the state of one cell can be affected by the state of
  another cell, depending on their distance. </li>
<li>how does the behavior of GoL change if one switches
  from a Moore to a von Neumann neighborhood (only vertical and
  horizontal adjacency)?</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Random-walkers">Random walkers<a class="anchor-link" href="#Random-walkers">&#182;</a></h1><p>The main difference between a cell and an <em>agent</em>, or <em>
  individual</em>, is that the agent can move in space. Particularly in
sociology and economics (and sometimes in ecology), agents are also
described as <em>purposeful</em>, in the sense that they try to maximize
some function of their own state variables, usually representing
fitness or gain.</p>
<p>Let us consider an agent living in discrete space and time. In
particular, it lives on a rectangular grid with von Neumann
neighborhoods. The agent is characterized by only two variables: its
coordinates on the $x$ and on the $y$ axis.  Suppose that the agent
starts at $x=0$, $y=0$, and call $a$ the distance between two
neighboring cells (the <em>step</em> of the lattice).</p>
<p>Simulate an agent that increases its own $x$ position at every odd
timestep, and its $y$ position at every even one.</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>what is the distance of the agent from its starting
  position, after 1, 2, 3 steps? what is its speed when
  $T\rightarrow\infty$?</li>
</ul>
<h2 id="Discrete-time">Discrete time<a class="anchor-link" href="#Discrete-time">&#182;</a></h2><p>Modify the agent behavior so that it performs a <strong>random walk</strong>: at
each time, it chooses a neighboring cell with uniform distribution
(implement a function for this), and moves
to it by updating its own $x$ or $y$ position. We are switching from a
<strong>deterministic</strong> to a <strong>stochastic</strong> model (<strong>Monte Carlo</strong>
method).</p>
<p>Assuming that all agents start at coordinates $(0,0)$, the agent's
expected r.m.s. distance from the origin after $T$ steps is:</p>
<p>\begin{eqnarray*}
  \langle d^2_T\rangle^{1/2} &amp; = &amp; \sqrt{\langle x_T^2 + y_T^2 \rangle} \\
                     &amp; = &amp; \sqrt{ \sum_{i=1}^T 4\cdot(1/4)\cdot a^2 } \\
 &amp; = &amp; a\sqrt T  
\end{eqnarray*}</p>
<p>Remember in fact that $\langle x_T \rangle = 0$, and so:
$$
\langle x_T^2\rangle = \langle \left(\sum_{i=1}^T x_i\right)^2\rangle
= \sum_{i=1}^T \langle x_i^2\rangle = T\,\left(P_++P_-\right)\,a^2
$$
where $P_+ = P_- = \frac14$. An analogous result can be derived from the variance of a binomial process in the case $P_+\neq P_-$.</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>how can you check for the scaling of the effective
  speed $v_{\rm eff} \equiv \sqrt{\langle d^2_T\rangle}/T$ with $T$,
  for example, using a linear regression method such as <code>numpy.linalg.lstsq</code>?</li>
<li>what does it mean if you don't find the correct
  powerlaw of $0.5$?</li>
<li>what is the expected r.m.s. speed of the walker? what
  is its instantaneous speed?  what is the "speed of light" in
  this model, i.e. the fastest speed at which a walker can reach
  some distance? what does this imply, if you give some real
  (physical) meaning to the space and time units of the simulation?</li>
</ul>
<h4 id="$\Longrightarrow$-See-random_walkers_DT.py.">$\Longrightarrow$ See <code>random_walkers_DT.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-See-random_walkers_DT.py.">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A stochastic process is a collection of random variables, indexed by
some discrete or continuous parameter (time), ${\bf X}(t)$. A <strong>realization</strong> 
of the process is the series of actual observed values
of these variables as a function of time. The <strong>ensemble</strong> is the
set of (some or) all possible realizations of the process.</p>
<p>Run simulations of 10, 100, 1000 random walkers together and check
again the scaling of the variance in their positions, which
corresponds to the r.m.s. distance from the origin calculated above.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>what can you learn if you build a distribution of
  scaling exponents from many realizations of the random walk?</li>
<li>it is not easy to visualize the walkers, especially
  close to the start of the simulation, when it is likely that many
  agents will occupy the same cells; how can you aggregate
  information about their state (position) in a way that is easier
  to visualize?</li>
<li>is there any difference between running a
  simulation with 100 random walkers, or running 100 simulations
  with one random walker each and combining the results
  appropriately? (independence, RNG seeding)</li>
</ul>
<p>The last ingredient we need to add to the random walkers is the <em>
  movement probability</em>. Rather than having all agents move at all
times, we can specify a new parameter of the model, $P_m$, which is
the same for all agents, representing the probability that each agent
will move in any of the possible directions in a timestep; $1-P_m$ is
the probability that it will stay where it is.  This introduces the
notions of <strong>macroscopic</strong> vs. <strong>microscopic</strong> description of a
system.</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>how does this new parameter affect the dispersal
  speed of the agents?</li>
</ul>
<p>In this case, our model now yields
\begin{equation}
\langle d^2\rangle^{1/2} = a \sqrt{P_m T}
\end{equation}</p>
<h2 id="Continuous-time">Continuous time<a class="anchor-link" href="#Continuous-time">&#182;</a></h2><p>Here we introduce the concept of <strong>waiting times</strong> in the *
  Gillespie algorithm} (Gillespie 1976, originally by Doob), and their
relation to the Poisson distribution.</p>
<p>Let's take the random walkers; suppose that we want to simulate the
same population of agents, but using different numerical timesteps. If
we want their macroscopic properties to be the same, independently of
how we choose the timestep of the simulation, $P_m$ will have to
depend on the duration of the timestep. Consider as an example two
models: model A with timestep $\tau$ and model B with timestep
$\tau' = 2\tau$. Looking at the random walk equations it may seem clear that, if
we want to have the same $\langle d^2\rangle$ at the same real time
(remember that the real -- macroscopic -- time elapsed is proportional
to $T$), in case B we will have had half the number
of timesteps of case A, so we should need <em>twice</em> the move
probability. In other words, the meaningful quantity for the model is
the <strong>rate</strong> of movement $m=P_m/\tau$, which is a <em>probability
  per unit time</em>; once you choose a time unit, you can derive the
correct probability to put into your ABM.</p>
<p>Is this an exact correspondence between models A and B? Look at the
probability that an agent moves *exactly once} within a time
$2\tau$ in model A:
$$
P({\rm 1\,move\,in\,2\,steps)} = P_m\cdot(1-P_m) + (1-P_m)\cdot P_m =
2\,P_m - 2\,P_m^2
$$
This means that the equivalence between models A and B is exact
  up to a term of the order of $P_m^2\propto\tau^2$. The smaller the
timestep (better time resolution), the better the correspondence
between models.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>to what kind of event does a probability $\propto P_m^2$ correspond?</li>
<li>how do you reconcile this with the scaling argument
  based on the random walk equations? (it was derived under the assumption of
  independence of all the terms in the sum) </li>
<li>is there any limitation due to the fact that
  probabilities have to be comprised between 0 and 1? any relation
  with what we called the "speed of light" of the model?</li>
</ul>
<p>What is the probability that a particle will make a single movement
within $T$ timesteps? The answer is given by the binomial distribution
for 1 success among $T$ trials (a.k.a.~Bernoulli distribution):
$$
P({\rm1\,move\,in\;}T{\rm\,steps}) =
\frac{T!}{1!\,(T-1)!}P_m\,(1-P_m)^{T-1}
$$
What happens if we now go to the <em>continuum limit</em>, i.e. we
consider models with smaller and smaller timesteps,
$\tau\rightarrow0$? This is the same as saying $P_m\rightarrow0$ and
$T\rightarrow\infty$; in this case, the binomial becomes a Poisson
distribution for 1 success in a time $t\equiv T\tau$ (which is a
sequence of Bernoulli trials):
$$
P({\rm1\,move\,in\,time\,}\tau) = m\,t\,{\rm e}^{-m\,t}
$$
where, again, $m$ is the rate of movement.  For this reason, this type
of events in continuous time is also called <em>Poisson point
  process</em>. One key feature of such process is the complete
independence of the events in different time intervals (the agents
have <em>no memory</em> and every time interval is statistically
equivalent). The expected number of moves performed by an agent within
a time interval $t$ is proportional to the length of the time interval
itself, and it is equal to $\langle N_{\rm moves}\rangle = m\,t$; it
does not depend on <em>when</em> we pick such interval.</p>
<p>What is the expected time before an agent decides to move (the 
  <strong>waiting time</strong> of this process)? In general the Poisson distribution
for $k$ successes in time $t$ reads
$$
P(k,t) = \frac{1}{k!}(m\,t)^k\,{\rm e}^{-m\,t}
$$
so the probability that the <em>first</em> move will be at $t_1$, <em>
  after</em> some time $t$, is equal to the probability that the agent
does not move before then:
$$
P(t_1 &gt; t) = P(k(t)=0) = \frac{1}{0!}(m\,t)^0{\rm e}^{-m\,t} = {\rm e}^{-m\,t}
$$</p>
<p>Implement a continuous-time ABM of random walkers in which each agent
waits for an exponentially distributed time before its next move. How
would you do that?</p>
<p>A few techniques for sampling variables from a continuous probability density:</p>
<ul>
<li>rejection sampling: given uniformly distributed $x$ and $u$,
reject $x$ if $u&gt;p(x)$</li>
<li>inverse transform: if you want $x$ distributed according to
$f(x)$ and $F(x)\equiv\int^xf(y){\rm d}y$, then take $x = F^{-1}(u)$
with $u$ uniformly distributed between 0 and 1, since
$$
{\rm Pr}(F^{-1}(u) \leq x) = {\rm Pr}(u\leq F(x)) = F(x)
$$
if needed, implemented via a look-up table</li>
<li>slice sampling and others (will not be used here)</li>
</ul>
<h4 id="$\Longrightarrow$-See-random_walkers_CT.py.">$\Longrightarrow$ See <code>random_walkers_CT.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-See-random_walkers_CT.py.">&#182;</a></h4><h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>how do the results from the continuous-time and the
  discrete-time ABM compare to each other? how would you
  quantitatively compare them?</li>
</ul>
<h2 id="Master-equation">Master equation<a class="anchor-link" href="#Master-equation">&#182;</a></h2><p>The Master equation describes the time evolution of a system that can
be in one of many <em>states</em>; this description is usually employed
for stochastic systems and involves <em>transition probabilities}
from one state to another. For example, in a system consisting of a
single random walker on a grid, the transition probabilities are
$P_m/4$ for the walker to move to each of the neighboring cells, and
$1-P_m$ for the walker to remain in the same state (not change its
position). Probability is always </em>conserved<em>. A stochastic process
in which the transition probabilities between the states at time $t$
and time $t+\tau$ depend only on the state of the system at time $t$
(i.e., without any effect of the previous history) is said to be a</em>Markov process<em>, or, in the specific case of discrete-time, a</em>Markov chain*.</p>
<p>In general, the Master equation can be written in many forms; we are
interested for the moment in the equation for a system with a discrete
set of states (for example, the positions of particles on a spatial
grid), that is
$$
\frac{{\rm d} p_i(t)}{{\rm d}t} = \sum_j\left[ W_{ij}p_j(t) -
  W_{ji}p_i(t) \right]
$$
where $p_j(t)$ is the probability that the system is in state $j$ at
time $t$, and $W_{ij}$ is the transition rate from state $j$ to state
$i$. In words, this equation says: <em>"the rate of change in the
  probability that the system will be in a particular state is equal
  to the sum of all possible transitions <strong>to</strong> this state
  minus all possible transitions <strong>away from</strong> this state''</em>.</p>
<p>In the case of the random walkers, the rate of change in the
probability that you will find agents in a particular cell is equal to
the probability that agents will walk <em>to</em> such cell from its
neighbors, minus the probability that agents already present in that
cell will walk <em>away</em> from it.</p>
<p>Once you specify some initial conditions in terms of a probability
distribution (e.g., a delta function centered on a particular state),
the stochastic evolution of the system can be followed with the Master
equation. For example, the <em>Leslie matrix</em> used in structured
population dynamics is actually an application of the Master equation.</p>
<p>The Master equation is extremely useful, for example in relating the
microscopic and the macroscopic properties of a system, as shown in
the next section.</p>
<h2 id="Diffusion-as-the-continuum-limit-of-random-walks">Diffusion as the continuum limit of random walks<a class="anchor-link" href="#Diffusion-as-the-continuum-limit-of-random-walks">&#182;</a></h2><p>Let's apply the master equation concretely to our discrete-time random
walkers. The probability of moving from a cell to one of its neighbors
is always $\frac14 P_m$ (we assume each cell to have 4 neighbors). So,
the change in probability that there are particles in cell $i$ (whose
neighbors are indicated by the index $j$) is
$$
\frac{p_i(t+\tau) - p_i(t)}{\tau} = \frac1\tau \left[\sum_{j} \frac14
  P_m p_j(t) - P_m p_i(t)\right]
$$
In the continuum limit ($\tau\rightarrow0$ and also the distance
between cells $a\rightarrow0$), calling $n(x,t)$ the expected density
of agents at point $x$ and time $t$,
\begin{equation}
\frac{{\rm d}n(x,t)}{{\rm d}t} = D\,\nabla^2n(x,t)
\end{equation}
where we have introduced the 2-dimensional {\bf macroscopic diffusion
  coefficient}
$$
D = \lim_{\tau,a\to0}\;P_m \frac14 \frac{a^2}{\tau}
$$
connecting the large-scale behavior of the ABM to the microscopic
movement probability and geometry of the cell
grid. This is the <strong>diffusion equation</strong>.</p>
<p>In order to derive it, remember that the finite-difference
approximation of the second derivative is (to lowest order):
$$
\left(\frac{{\rm d}^2 f(x)}{{\rm d}x^2}\right)_{x=x_i} \approx
\frac{f(x_{i+1}) + f(x_{i-1}) - 2 f(x_i)}{(\Delta x)^2}
$$
where $\Delta x = (x_{i+1}-x_i) = (x_i-x_{i-1})$.</p>
<p>The solution to the
diffusion equation applied to the density of a group of agents (all
starting their movement from the same point in space) is a
(multi-variate) Gaussian whose variance $\sigma^2$ increases linearly
with time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Population-growth">Population growth<a class="anchor-link" href="#Population-growth">&#182;</a></h1><p>Let's introduce birth and death into a population of agents. The
number of agents as a function of time, $N(t)$, will be
$$
N(t+\tau) - N(t) = B(N,t,\tau,...) - D(N,t,\tau,...)
$$
where $B$ and $D$ are, respectively, the number of births and deaths
within time $\tau$.  In analogy with the movement rate, we can define
the <strong>growth rate</strong> of a population as
$$
r(N,t,\tau,...) \equiv \frac1\tau\,\frac{N(t+\tau) - N(t)}{N(t)} =
\frac1\tau\frac{B(N,t,\tau,...)-D(N,t,\tau,...)}{N(t)}
$$</p>
<p>Consider a stochastic, continuous-time ABM, in which the <em>per
  capita</em> rates for giving birth and/or dying are $b(N,t,...)$ and
$d(N,t,...)$:
\begin{eqnarray*}
  \langle B(N,t,\tau,...)\rangle &amp; = &amp; \langle b(N,t,...)\,N(t)\,\tau\rangle \\
  \langle D(N,t,\tau,...) \rangle &amp; = &amp; \langle d(N,t,...)\,N(t)\,\tau\rangle
\end{eqnarray*}
assuming that birth and death events are uncorrelated. The expected
growth rate is then 
$$
\langle r \rangle = \langle b-d\rangle\;.
$$</p>
<p>In continuous time, we can assume that the probability of having more
than one birth or death at a single instant is negligible. Then,
according to the master equation:
\begin{eqnarray*}
 \frac{{\rm d}P(N(t))}{{\rm d}t} &amp; = &amp; b(N-1)\,(N-1)\,P(N-1,t) \\
                                   &amp; &amp; +d(N+1)\,(N+1)\,P(N+1,t) \\
                                   &amp; &amp; -[b(N)+d(N)]\,N\,P(N,t)
\end{eqnarray*}
from which we get
$$
\left\langle\frac{{\rm d}N(t)}{{\rm d}t}\right\rangle= \langle
bN\rangle - \langle dN\rangle
$$
We could write a (more complex) Master equation also for a
discrete-time ABM of the same type, remembering that we can have many,
binomially distributed births and deaths within a timestep $\tau$.</p>
<h2 id="Exponential">Exponential<a class="anchor-link" href="#Exponential">&#182;</a></h2><p>If a population has a constant growth rate,
$$
N(t+\tau) = N(t)\cdot(1 + r\,\tau)
$$</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>implement two discrete-time ABMs with constant growth
  rate, one deterministic, the other stochastic</li>
</ul>
<p>It is easy to integrate analytically the continuum limit
($\tau\rightarrow0$)
\begin{equation}
\frac{{\rm d\,}N(t)}{{\rm d}\,t} = r\,N(t)
\end{equation}
(add expectation values if the model is stochastic!) to obtain
$$
N(t) = N(0)\cdot{\rm e}^{r\,t}
$$
that is, exponential growth, stationarity, or decline: $r\leq0$ is
also allowed.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>implement a continuous-time ABM of exponential growth
  and compare it with the analytical solution; for this model, you
  can use the random walkers that you have already implemented: why?
  (because they are all independent, and therefore space and
  movement have no effect on growth)</li>
<li>how would you integrate the equation for exponential growth
  numerically? what comparisons can you draw with discrete- and
  continuous-time ABMs?</li>
</ul>
<h2 id="Logistic">Logistic<a class="anchor-link" href="#Logistic">&#182;</a></h2><p>No population can grow indefinitely; some factor will intervene to
slow down and eventually halt growth. The population may reach a
"saturation" point and stay there, in equilibrium. This stable
point is often quantified with a parameter called <strong>carrying
  capacity</strong> $K$, which represent a sustainable population size or
density.</p>
<p>In classical population models, this is usually described by logistic
population growth:
\begin{equation}
\frac{{\rm d}N}{{\rm d}t} = r\,N\,\left(1-\frac N K \right) 
\end{equation}
This equation has a sigmoid or exponential analytical solution
(depending if $N(0)&lt;K$ or $&gt;K$), has stable points at $N=0$ and $N=K$,
and looks like exponential growth for small populations (this is why
$r$ is sometimes called "zero-population growth rate"). The sigmoid
solution is:
$$
N(t) = \frac{K\,N(0)}{(K-N(0))e^{-rt}+N(0)}\;.
$$</p>
<p>If you consider logistic growth in discrete time (as a 
<em>recurrence relation</em>), it displays notriously a "deterministically
chaotic" behavior; this is the case for real populations with
non-overlapping generations ("Simple mathematical models with very
complicated dynamics", R.~M.~May, Nature, 1976).</p>
<p>Note that there are two r.h.s. terms: one term $\propto N$ and the
other $\propto N^2$. The first can be interpreted as an "intrinsic"
characteristic of each agent, the second some kind of "interaction"
between the agents, in this case with a negative effect on population
growth.</p>
<p>There are many ways to implement logistic growth as an ABM. For
example, we cast the logistic growth equation into a large-scale
expectation value of some agent-level behavior, by considerig the
expectation value of the <em>per capita</em> growth rate (i.e., the
contribution of each individual to population growth):
$$
\left&lt;\frac1N\frac{{\rm d}N(t)}{{\rm d}t}\right&gt; =
\langle\alpha\rangle - \langle\beta\,N(t)\rangle
$$ 
Assuming $\alpha=r$ and $\beta=r/K$ constant, the rates at which an
agent gives birth and dies, respectively, can be given by the Verhulst
model:
\begin{eqnarray*}
  \tilde b &amp; = &amp; r \\
  \tilde d(N) &amp; = &amp; r\,\frac{N(t)}{K}
\end{eqnarray*}
This may be interpreted as a constant fertility of the agents,
combined with a density-dependent death rate (for example, due to
competition for limited resources). But this is just one of many
possible interpretations.</p>
<p>These processes are sometimes expressed as follows in analogy with
chemical reactions, with their respective probabilities:
$$
A \xrightarrow{\beta} A + A
$$
$$
A + A \xrightarrow{\delta} A 
$$
and in the same way, "intrinsic" death would be
$$
A \xrightarrow{\delta'} \emptyset
$$</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>try an ABM in which the self-interaction term
  corresponds to agents killing others with a certain probability;
  plus, the agents have intrinsic birth and death probabilities;
  where does the "natural death" probability enter in the
  large-scale logistic equation?</li>
<li>code ABMs of logistic growth with "linear death"</li>
</ul>
<p>What happens to our large-scale growth rate in the Verhulst
formulation?  Remember that
$\langle N^2\rangle = \langle N\rangle^2 + \sigma_N^2$ where
$\sigma_N$ is the variance in population size. Therefore, the
expectation of the ABM growth rate is actually
$$
\left&lt; \frac{{\rm d}N(t)}{{\rm d}t}\right&gt; = \alpha\langle
N(t)\rangle - \beta\langle N(t)\rangle^2 - \beta\,\sigma_N^2
$$
i.e., with this stochastic ABM we are <em>underestimating</em> the growth
rate compared to the classical model. The equilibrium population size
is also lower than the classical $K$.  This is in addition to any
discrepancy due to your choice of timestep.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Fisher-Kolmogorov-Skellam-populations">Fisher-Kolmogorov-Skellam populations<a class="anchor-link" href="#Fisher-Kolmogorov-Skellam-populations">&#182;</a></h1><p>When random walk and logistic growth are combined into a single ABM,
we get a discrete, stochastic version of the popular
Fisher-Kolmogorov-Skellam population model.</p>
<h4 id="$\Longrightarrow$-See-verhulst_DT.py.">$\Longrightarrow$ See <code>verhulst_DT.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-See-verhulst_DT.py.">&#182;</a></h4><h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>what <strong>emergent property</strong> do you see in the population?</li>
<li>how would you estimate the most important quantity
  related to such emergent phenomenon, using the parameters of the
  model?  (concepts of <em>dimensional analysis</em> and <em>scaling relations</em>)</li>
<li>test an ABM with constant birth and death probability,
  in which only one agent at a time can occupy a cell; offspring is
  created only if space is available in a neighboring cell</li>
<li>carrying capacity can be an emergent property of the system, 
  instead of being one of its input parameters; can you think of a way
  to have it arise in a population?</li>
</ul>
<h4 id="$\Longrightarrow$-See-exclusion_CT.py-and-killers_CT.py-for-examples-of-emergent-carrying-capacity.">$\Longrightarrow$ See <code>exclusion_CT.py</code> and <code>killers_CT.py</code> for examples of emergent carrying capacity.<a class="anchor-link" href="#$\Longrightarrow$-See-exclusion_CT.py-and-killers_CT.py-for-examples-of-emergent-carrying-capacity.">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Resources-and-environment">Resources and environment<a class="anchor-link" href="#Resources-and-environment">&#182;</a></h1><p>Suppose that large-scale carrying capacity arises from the balance
between resource production and use by a single species; how would you
estimate an expected $K$ from the parameters of an ABM designed to
test this hypothesis? Implement an ABM for "cows and grass" and compare
to your expectations.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>random walkers with BMR vs. resource growth evolve their
population-averaged movement rate (at birth, each cow inherits the
mother agent's rate plus/minus some random variation); do they
settle around an "optimum" move probability? what happens if you
add an energetic cost to each movement action?</li>
<li>what happens if you add the restriction that an agent
<em>needs another agent at the same spatial location</em> in order to
reproduce (a version of the <strong>Allee effect</strong>)?</li>
</ul>
<h4 id="$\Longrightarrow$-See-cows_grass_DT.py-and-cows_grass_CT.py.-For-a-more-complex-model,-check-out-cows_grass_evo_CT.py-in-which-the-cow-population-evolves-its-movement-probability-parameter-with-time.">$\Longrightarrow$ See <code>cows_grass_DT.py</code> and <code>cows_grass_CT.py</code>. For a more complex model, check out <code>cows_grass_evo_CT.py</code> in which the cow population evolves its movement probability parameter with time.<a class="anchor-link" href="#$\Longrightarrow$-See-cows_grass_DT.py-and-cows_grass_CT.py.-For-a-more-complex-model,-check-out-cows_grass_evo_CT.py-in-which-the-cow-population-evolves-its-movement-probability-parameter-with-time.">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Predators-and-prey">Predators and prey<a class="anchor-link" href="#Predators-and-prey">&#182;</a></h1><p>In the classical Lotka-Volterra model, the "self-interaction" term of
logistic growth becomes a pairwise <em>inter-specific interaction</em>
term.  In a two-species system, we assume that:</p>
<ul>
<li>in the absence of species B (predator), species A (prey) should thrive</li>
<li>in the absence of A, B should die out</li>
</ul>
<p>In ABM terms, we can give to the species the following actions: a <em>sheep</em> 
(A) moves and gives birth (e.g., with constant rates), while
a <em>wolf</em> (B) moves and feeds (if feeding is successful, wolf
reproduces and target sheep dies; if it is not successful, wolf
dies). A model with more long-term oscillations would be to give
wolves an energy content, which increases at every successful feeding
and decreases with time and actions; a zero or negative energy leads
to the wolf's death. Anisotropic movement can be introduced for sheep,
wolves, or both; in the case of sheep, how would you introduce a
preference for <em>low</em> wolf densities?</p>
<h4 id="$\Longrightarrow$-Try-running-predprey_DT.py-a-few-times-and-check-for-different-emerging-behaviors!">$\Longrightarrow$ Try running <code>predprey_DT.py</code> a few times and check for different emerging behaviors!<a class="anchor-link" href="#$\Longrightarrow$-Try-running-predprey_DT.py-a-few-times-and-check-for-different-emerging-behaviors!">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Epidemics:-the-S-I-R-model">Epidemics: the S-I-R model<a class="anchor-link" href="#Epidemics:-the-S-I-R-model">&#182;</a></h1><p>To study the spread of disease in a population, we use agents
characterized by a 3-state variable representing their health state,
which can take the values <em>susceptible</em>, <em>infected</em>, or 
<em>recovered</em> (and immune to further contagion). Let's call $\beta$ the
rate at which each of the $N$ total individuals go from susceptible to
infected (in dependence of contact with other infected individuals
within some <em>infection radius</em>, giving rise to a pairwise
interaction term), and $\gamma$ the rate at which infected individuals
recover and become immune to the disease.</p>
<p>In classical dynamics of non-spatial epidemics, calling the number of
individuals in each of these states as $S$, $I$, and $R$,
respectively, this standard model corresponds to the following time
evolution:</p>
<p>\begin{eqnarray*}
  \frac{{\rm d} S} {{\rm d}t} &amp; = &amp; -\beta \frac{S\cdot I}{N} \\
  \frac{{\rm d} I} {{\rm d}t} &amp; = &amp; \beta \frac{S\cdot I}{N} - \gamma\,I \\
  \frac{{\rm d} R}{ {\rm d}t} &amp; = &amp; \gamma\,I \\
\end{eqnarray*}</p>
<p>In the classical model, the parameter $R_0\equiv\beta/\gamma$
determines if there will be an epidemic ($R_0&gt;N/S(0)$). A typical
application of this model is to the Hong Kong Flu that hit New Yorkers
in 1968-1969, with a contageous period of 3 days and a 12-week
duration of the epidemic.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>the classical model assumes that the population is 
<em>well-mixed</em>: you can check the classical solution by using the
<code>odeint()</code> function in <code>scipy.integrate</code>; how does the ABM
differ from the equation-based expectations, as a function agent
density and movement rate?</li>
<li>experiment with some small level of <em>inoculation</em>
against the disease, which adds the possibility that an agent
switches directly from <em>susceptible</em> to <em>recovered</em> directly</li>
<li>modify this system into a <strong>zombie apocalypse</strong>
scenario: which behavioral rules would you add / change / remove?</li>
<li>the S-I-R model can be modified into a S-I-R-S, or a
S-I-S model, among others; what changes do you expect in the
population's behavior?</li>
</ul>
<h4 id="$\Longrightarrow$-Compare-the-outcomes-of-contagion_DT.py-and-classic_SIR.py.">$\Longrightarrow$ Compare the outcomes of <code>contagion_DT.py</code> and <code>classic_SIR.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-Compare-the-outcomes-of-contagion_DT.py-and-classic_SIR.py.">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Movement-and-environment">Movement and environment<a class="anchor-link" href="#Movement-and-environment">&#182;</a></h1><h2 id="Environment-perception">Environment perception<a class="anchor-link" href="#Environment-perception">&#182;</a></h2><p>By introducing a non-homogeneous enviroment, we have a chance to
explore non-isotropic movement strategies. In order to do so, we use
the <em>"Roulette wheel"</em> algorithm. We use a function $W(...)$
that converts some property of each spatial location $i$ (e.g.,
density of grass in a cell) into a statistical weight $W_i$. Then, the
probability of moving to cell $i$ is $P_i\propto W_i$. By using the
standard random variable $u$ uniformly generated in the $[0,1)$
interval, the agent will decide to move to neighboring cell $i$ (with
$i = 1, 2, 3, 4$ and $W_0=0$) if
$$\sum_{k=0}^{i-1} W_k\leq u &lt; \sum_{k=0}^{i}W_k\;.$$
Of course, the sum of the weights for all possibilities presented to
an agent has to be normalized to 1.  It is important to note that this
is just another version of the <strong>inverse transform sampling</strong>: we
are using the sum, i.e. the (discrete) integral of the statistical
weights (probabilities), to sample the agent's probability distribution
of moving to each neighboring cell.</p>
<p>The weight function can be of any form, but keep in mind that constant
multiplicative factors will always be normalized away. A good choice
would be a weight of the form $w_i = a + b\cdot G_i^\alpha$, where
$G_i$ is the grass content (or some other property) of cell $i$ and
$\alpha$ is a <em>non-linearity parameter</em> ($\alpha&gt;1$ biases cows
more strongly towards places with more grass, while $\alpha&lt;1$ tends
towards the random movement $\alpha=0$ limit); a high $a/b$ makes the
movement less biased and more isotropic.</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>what happens when you introduce anisotropic movement
into the cows--grass system? do you expect that it will increase or
decrease the carrying capacity? how does it change the spatial
distribution and the time evolution of the population?</li>
</ul>
<h4 id="$\Longrightarrow$-Check-out-cows_grass_aniso_DT.py,-where-cows-tend-to-move-towards-higher-grass-densities,-and-compare-the-large-scale-system-behavior-with-that-of-cows_grass_DT.py,-where-cows-move-&quot;blind&quot;.">$\Longrightarrow$ Check out <code>cows_grass_aniso_DT.py</code>, where cows tend to move towards higher grass densities, and compare the large-scale system behavior with that of <code>cows_grass_DT.py</code>, where cows move "blind".<a class="anchor-link" href="#$\Longrightarrow$-Check-out-cows_grass_aniso_DT.py,-where-cows-tend-to-move-towards-higher-grass-densities,-and-compare-the-large-scale-system-behavior-with-that-of-cows_grass_DT.py,-where-cows-move-&quot;blind&quot;.">&#182;</a></h4><p>When spatial or temporal patterns arise, two of the most common
analyses you can perform are the <em>2-point correlation function</em>
and the <em>Fourier decomposition</em>. The latter is a standard analysis
whose details can be found in many places, and is implemented in the
<code>numpy.fft</code> library. The former measures the excess probability of
finding two agents at some mutual distance, relative to the
expectation due to a random distribution of agents on the same
spatial domain. A simple
formulation is the Landy-Szalay equation
$$
\xi(r) = \frac{DD - 2DR + RR}{RR}
$$
where $DD$ indicates the probability of finding two agents ("D" for
"data") at distance $r$ in your simulation (computed among all
possible pairs of agents), $RR$ the probability of finding two random
points at the same distance ("R" for "random", and $DR$ the
cross-correlation of agents and random sample. The "R" sample
correspond to generating a number (at least $N_{\rm agents}$ of points
randomly and uniformly distributed on your grid (as you may do, e.g.,
for your initial conditions).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Self-organization-and-collective-motion">Self-organization and collective motion<a class="anchor-link" href="#Self-organization-and-collective-motion">&#182;</a></h2><p>Self-organization can arise by including some form of <em>memory</em> in
the agents or their environment, which is at the basis of an
autocatalytic reaction. For the grass-cow system, this memory is
"recorded" in the grass density.  In the case of ant colonies,
pheronome trails mediate a positive feedback loop between the
trajectories of ants at different times. A classical ABM of forager
ant self-organization (Goss et al.~1989, Deneubourg et al.~1990),
models ants as agents characterized by their position on a 2-d
"floor" and by an internal state that can be either <em>searching</em>
or <em>not searching</em> (for food). Ants start their journey at the
colony the ($x=0$, $y=0$), and food is situated on the opposite corner
and at random locations. Ants that are <em>searching</em> can only
increase their $x$ or $y$ position (moving away from the colony); in
the original papers, the probability of increasing $x$ is a slightly
modified version of the roulette wheel algorithm:
$$
P_{+x} = \frac{(k + N_{+x})^n}{(k+N_{+x})^n + (k+N_{+y})^n}
$$
where $N_{+x}$ is the number of ants that have already gone through
that cell; and analogously for the $+y$ choice. The information about
previous ants is "saved" on the floor in the form of a pheromone
trail. The parameter $k$ guarantees that relatively unexplored areas
are approached with a roughly random choice, when little information
has been left by previous ants; the parameter $n$ determines the
degree of non-linearity of the choice when information is available.
When an ant reaches food, it switches to the <em>not searching</em>
state; ants that are <em>not searching</em> can only go back in $x$ or
$y$ until they get to the colony.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>a best fit to the data in the original paper gives
$k=20$ and $n=2$; compare with a random walk ($n=0$) and with the
original model for weighted movement </li>
<li>how does the system behavior change by making it more
realistic: pheromones fade with time and food is exhausted after
repeated visits?</li>
<li>code an evolutionary ABM (e.g., in which ants die if
they do not reach food within a certain number of moves) and see if
the system approaches an optimum configuration</li>
<li>is there any difference between an ABM in which all ants
start at the same time, and one in which ants are gradually
"injected" into the system from the origin?</li>
</ul>
<h4 id="$\Longrightarrow$-See-ants.py.">$\Longrightarrow$ See <code>ants.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-See-ants.py.">&#182;</a></h4><p>Flocks of birds and schools of fish exhibit collective motions, in
which all individuals seem to agree on the direction followed by the
group. This same level of self-organization can be achieved without
memory, by using a short-range interaction ABM and some kind of 
<strong>positive feedback loop</strong>. Consider a population in which each agent
moves at the same speed $v$ and reacts to its neighbors' behavior by
adjusting its own direction of motion $\theta_i$:
$$
\theta_i(t+\tau) = \langle \theta(t) \rangle_{\rm neighbors} + \eta
$$
where $\eta$ is some random noise term (uniform, gaussian\dots). Note
that you may have to use a function such as <code>math.atan2(s,c)</code> to
compute $\langle\theta\rangle$.  As you can see, one of the collective
effects of flocking behavior is that a <strong>consensus</strong> is reached, and
spatial clusters (i.e., flocks) may arise.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>study the emergent behavior of a 2-dimensional group of
agents as a function of how the neighborhood is defined, and how
large $\eta$ is (concept of <strong>phase transition</strong>); do you expect
flocks to arise when the "neighborhood" is large, or small?</li>
<li>refine the ABM by including the fact that agents can
only see the neighbors that they have in front (cutoff angle)</li>
</ul>
<h4 id="$\Longrightarrow$-See-flock_DT.py-for-a-basic-model-of-bird-oids.">$\Longrightarrow$ See <code>flock_DT.py</code> for a basic model of bird-oids.<a class="anchor-link" href="#$\Longrightarrow$-See-flock_DT.py-for-a-basic-model-of-bird-oids.">&#182;</a></h4><p>We can modify the agents' behavioral rule slightly, by asking that
they try to avoid close contact with others. This can be done by using
a very short-range <em>repulsive force</em> between the agent.  If you
add a preferred direction for each agent, this type of model is used
to reproduce movement patterns of crowds of pedestrians.</p>
<h2 id="L&#233;vy-walks-and-foraging">L&#233;vy walks and foraging<a class="anchor-link" href="#L&#233;vy-walks-and-foraging">&#182;</a></h2><p>A Lévy walk (or Lévy flight) is a superdiffusive motion, in which
the probability distribution of spatial steps is "heavy
tailed". Here we focus on a particular case of Levy walk, for which
the cumulative probability of step length $a$ is:
$$
P(a&lt;l) = \left\{
\begin{array}{lr}
0 &amp; l&lt;L \\
1-\left(\frac l L\right)^{-d} &amp; l\geq L
\end{array}
\right.
$$
where $L$ is the minimum length of a spatial step and $0&lt;d&lt;2$. This
implies that the probability density for step lengths above $L$ is
$P(a) \propto a^{-\mu}$ with $1&lt;\mu&lt;3$ (higher $\mu$ converges to
Brownian motion, lower $\mu$ cannot be normalized).  The Lévy walk
also seems to reproduce some statistical properties of the movement
<strong>patterns</strong> of many foraging species (bees, albatross, sharks,
humans, amoebas), when positions are recorded at fixed time intervals
(i.e., they imply changes in movement speed). However, it is highly
debated whether they arise from a Lévy-like <strong>process</strong>, or they
are an artefact due to time sampling and data binning. An alternative
explanation for such patterns can be that the forager switches between
two random walks (or more generally Brownian Motion) with different
step sizes, a large one for <em>searching</em> mode and a small one for
<em>feeding</em> mode (when in a patch).</p>
<p>We can use this distribution to generate spatial steps for our agents
with the usual inverse transform sampling: $a = L\cdot u^{-1/d}$ where
$u$ is our uniform random variable. The direction of the step will
have to be chosen independently.</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>compare a Levy walk with a <strong>random walk with
  persistence</strong> (also called <strong>correlated random walk</strong>), i.e. a
random walk in which the agent is more likely to go in the same
direction as the previous step(s); what are your expectations before
you run the ABM?</li>
</ul>
<p>Consider agents with a Lévy-flight movement strategy, who are
foraging in an environment where resources have a sparse, "patchy"
distribution. You can define a <strong>search efficiency</strong> as some measure
of the ratio between the number of visited patches --- or the total
collected food --- and the distance travelled, either in total or
between each patch visit.</p>
<h4 id="Questions">Questions<a class="anchor-link" href="#Questions">&#182;</a></h4><ul>
<li>how would you set up such an ABM, assuming you are using
a spatial grid for the environent?</li>
<li>with an adaptive population of agents for whom movement
costs energy, find out if there is an optimal value of the Lévy
scaling exponent $d$ (maximum efficiency), depending on the degree
of patchiness in the environment</li>
</ul>
<h4 id="$\Longrightarrow$-A-basic-L&#233;vy-flight-model-in-continuous-time-is-implemented-in-levy.py.">$\Longrightarrow$ A basic L&#233;vy flight model in continuous time is implemented in <code>levy.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-A-basic-L&#233;vy-flight-model-in-continuous-time-is-implemented-in-levy.py.">&#182;</a></h4><p>The <strong>marginal value theorem</strong> (Charnov 1976) applies to the case of
foraging with <em>diminishing returns</em> (i.e., the longer you stay in
the same patch, the lower your food collection rate becomes): the
predator should leave the patch when the marginal capture rate in the
patch (i.e., the rate of energy assimilation minuts the rate of energy
expenditure) drops to the average rate of patch encounters in the
habitat.</p>
<h4 id="Question">Question<a class="anchor-link" href="#Question">&#182;</a></h4><ul>
<li>how would you test the marginal value theorem with a stochastic ABM?</li>
</ul>
<h1 id="Schelling's-segregation-model">Schelling's segregation model<a class="anchor-link" href="#Schelling's-segregation-model">&#182;</a></h1><p>Schelling's 1971 segregation model was a pioneering work in showing
how collective behavior can emerge from simple agent-level rules. In
it, agents belonging to one of two groups live on a grid with Moore
neighborhood. An agent is <em>happy</em> about
its current position if the fraction of neighbors belonging to its
same group is $f\geq R$, where $R$ is an <em>intolerance parameter</em>.</p>
<p>Different update rules are possible. For example, at
each iteration of the model, a random agent is considered, and, if
unhappy, it is moved to a random empty location on the grid. The
emergent result is that strong spatial segregation of group members
can happen even with what could be considered a low intolerance level
of each individual.</p>
<h4 id="$\Longrightarrow$-See-schelling.py.">$\Longrightarrow$ See <code>schelling.py</code>.<a class="anchor-link" href="#$\Longrightarrow$-See-schelling.py.">&#182;</a></h4>
</div>
</div>
</div>
    </div>
  </div>
</body>

 


</html>
